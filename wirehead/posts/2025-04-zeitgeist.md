# Zeitgeist: The Agent Arrives

## April 2025 was when AI stopped being conversational and started being operational - the month intelligence learned to touch the world through interfaces, orbit Earth on spacecraft, and charge premium rates for doing both.

March had delivered multimodal perception. April brought deployment at scale.

The month opened with [Meta releasing Llama 4 Scout on April 5](https://ai.meta.com/blog/llama-4-multimodal-intelligence/) - the first natively multimodal open-source model that could process text, images, and audio without architectural seams. Not a bolt-on vision module or audio adapter, but intelligence that thought across modalities from the ground up. The [17 billion active parameter model with 16 experts](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E) matched proprietary systems while remaining downloadable, forkable, deployable on hardware you could own. The democratization accelerated: frontier capabilities escaping the API economy, becoming infrastructure anyone could run.

But democratization had its inverse. [Anthropic announced Claude Max on April 9](https://techcrunch.com/2025/04/09/anthropic-rolls-out-a-200-per-month-claude-subscription/) - a $200-per-month subscription tier with 20x higher rate limits than the standard Pro plan. Following OpenAI's playbook, the pricing signaled a new market segment: power users willing to pay premium for priority access and higher throughput. The AI economy bifurcating - open weights flowing downward, premium tiers climbing upward, the middle ground thinning. Anthropic's product lead wouldn't rule out [$500-per-month plans](https://techcrunch.com/2025/04/09/anthropic-rolls-out-a-200-per-month-claude-subscription/) in future. Intelligence as luxury good, reasoning as status symbol.

The infrastructure beneath this bifurcation continued its exponential build. [Google unveiled Ironwood at Cloud Next on April 23](https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/) - their seventh-generation Tensor Processing Unit, the first designed specifically for the inference era. Not training new models but running them at scale, serving billions of queries, the operational phase of the AI lifecycle. [42.5 Exaflops when scaled to 9,216 chips](https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/), more than 24 times the compute power of the world's largest supercomputer. The numbers had become abstract, but the direction was clear: inference workloads growing faster than training, deployment outpacing development, the shift from research to production infrastructure.

April's defining moment came through [Microsoft's quiet announcement in Copilot Studio](https://www.microsoft.com/en-us/microsoft-copilot/blog/copilot-studio/whats-new-in-copilot-studio-april-2025/ "What's new in Copilot Studio: April 2025"): [computer use](https://www.microsoft.com/en-us/microsoft-copilot/blog/copilot-studio/announcing-computer-use-microsoft-copilot-studio-ui-automation/ "Announcing computer use in Microsoft Copilot Studio"). AI agents could now interact with any graphical user interface - navigating menus, clicking buttons, typing in fields. ["If a person can use the app, the agent can too,"](https://www.microsoft.com/en-us/microsoft-copilot/blog/copilot-studio/whats-new-in-copilot-studio-april-2025/ "Charles Lamanna on computer use capabilities") said Charles Lamanna, CVP of Business & Industry Copilot. The boundary dissolved between AI-native applications and legacy software. Every interface became an API, every workflow a potential automation target. The agent didn't need custom integrations or specialized connectors - just vision, reasoning, and the ability to manipulate pixels like a human would.

The implications materialized across enterprise deployments. New [Microsoft Graph connectors for Guru, GitLab, Asana, Miro, Trello, Zendesk](https://www.microsoft.com/en-us/microsoft-copilot/blog/copilot-studio/whats-new-in-copilot-studio-april-2025/ "New Microsoft Graph connectors in Copilot Studio") - the connective tissue between AI and business systems, [agents reaching into knowledge bases and project management tools](https://learn.microsoft.com/en-us/graph/connecting-external-content-connectors-overview "Microsoft Graph connectors overview"), reasoning across organizational silos. The agent architecture spreading through corporate infrastructure like mycelium through soil.

Then [Llama went to space](https://about.fb.com/news/2025/04/space-llama-metas-open-source-ai-model-heading-into-orbit/ "Space Llama: Meta's Open Source AI Model Is Heading Into Orbit"). On April 25, [Booz Allen deployed a fine-tuned version of Llama 3.2](https://www.boozallen.com/menu/media-center/q1-2026/booz-allen-and-meta-space-llama.html "Booz Allen and Meta collaborate on Space Llama") aboard the International Space Station National Laboratory. Not a publicity stunt but operational necessity - [astronauts needed AI that could run without internet connectivity](https://about.fb.com/news/2025/04/space-llama-metas-open-source-ai-model-heading-into-orbit/ "Why Llama for space deployment"), process technical documents, assist with repairs and maintenance in an environment where latency to Earth-based servers could mean mission failure. [The tech stack combined Booz Allen's A2E2, HPE's Spaceborne Computer-2, and NVIDIA acceleration](https://about.fb.com/news/2025/04/space-llama-metas-open-source-ai-model-heading-into-orbit/ "Space Llama technical architecture"), compressing AI inference from minutes to just over a second. Intelligence at the edge of space, reasoning in low orbit, the first AI system operating beyond Earth's atmosphere.

The symbolism was almost too perfect: open-source models escaping planetary boundaries while premium subscriptions climbed toward $200 monthly. AI simultaneously democratizing and stratifying, becoming both ubiquitous infrastructure and luxury service, operational tool and status marker.

April's funding patterns reflected this duality. [Enterprise AI adoption continued its exponential curve](https://openai.com/index/the-state-of-enterprise-ai-2025-report/ "The state of enterprise AI"), but the money flowed toward [deployment infrastructure rather than model development](https://www.deloitte.com/us/en/insights/industry/power-and-utilities/data-center-infrastructure-artificial-intelligence.html "AI infrastructure investment trends"). The research phase giving way to the operational phase, the age of training yielding to the age of inference. [Google's Ironwood represented this pivot in silicon](https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/ "Ironwood: The first Google TPU for the age of inference") - chips optimized not for discovering new capabilities but for serving existing ones at massive scale.

The agent architecture that emerged through April wasn't the autonomous AGI of science fiction. It was something more mundane and more significant: AI that could navigate existing interfaces, operate in disconnected environments, scale across enterprise workflows, and charge premium rates for priority access. Intelligence becoming operational infrastructure, reasoning becoming a service layer with tiered pricing.

The month ended with the contours of the next phase visible. Not breakthrough capabilities but deployment at scale. Not new model architectures but agents that could touch existing systems. Not research papers but production infrastructure. April 2025 was when AI stopped being a research problem and became an operations challenge - when the agent arrived not as science fiction but as enterprise software, space-based edge compute, and $200-per-month subscription tiers.

The future had arrived wearing the clothes of business software, running on spacecraft, and accepting credit cards.
