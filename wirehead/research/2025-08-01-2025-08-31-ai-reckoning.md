# AI's Environmental and Ethical Reckoning: Transparency, Misuse, and the Copyright Battle

*August 2025 brought unprecedented transparency on AI's energy costs, documented cases of AI-enabled crime, and new approaches to the publisher compensation problem*

## Google Breaks the Silence on Energy

On August 21, 2025, Google did something no major AI company had done before: it published detailed data on how much energy its AI systems actually consume.

The numbers were striking. A median Gemini prompt uses 33 times more energy than a traditional Google search query. The technical report, published on arXiv, provided "the most transparent estimate yet from one of the big AI companies, and a long-awaited peek behind the curtain for researchers," MIT Technology Review reported.

The disclosure came as AI's environmental footprint faced increasing scrutiny. With companies like OpenAI and President Trump announcing the $500 billion Stargate initiative to build massive AI data centers—each potentially requiring five gigawatts of power, more than the entire state of New Hampshire—the energy question had become impossible to ignore.

Google's methodology was notably rigorous. Rather than using emissions estimates based on US grid averages, the company calculated actual energy consumption and water usage for its specific infrastructure. The transparency set a new standard for the industry.

"Understanding AI's energy use was a huge global conversation" in 2025, MIT Technology Review noted in its year-end review. Google's August disclosure provided the first solid data point from a major player.

## The Dark Side: Anthropic's Threat Report

Six days after Google's energy disclosure, Anthropic released a very different kind of transparency: its August 2025 Threat Intelligence Report documenting how criminals are misusing Claude.

The cases were alarming:

**A large-scale extortion operation** used Claude Code to automate threatening communications to victims. The AI's ability to generate personalized, convincing messages at scale made the operation more effective than traditional approaches.

**A North Korean fraudulent employment scheme** leveraged Claude to create convincing fake identities and job applications, part of the regime's ongoing efforts to place operatives in Western companies.

**AI-generated ransomware** created by a cybercriminal with only basic coding skills. "This actor appears to have been dependent on AI to develop functional malware," Anthropic noted—a troubling demonstration of how AI lowers the barrier to cybercrime.

"We've developed sophisticated safety and security measures to prevent the misuse of our AI models," Anthropic stated. "But cybercriminals and other malicious actors are actively attempting to find ways around them."

The report represented an unusual level of transparency about AI misuse. Most companies prefer to downplay security incidents. Anthropic's decision to publish detailed case studies reflects its positioning as a safety-focused lab—and perhaps recognition that the industry needs to confront these issues openly.

## The Copyright Battlefield

August also saw continued evolution in the contentious relationship between AI companies and content creators. With 51 active copyright lawsuits against AI companies and no resolution in sight, the industry sought alternative approaches.

Perplexity, the AI search startup, announced a new subscription model called Comet Plus that shares revenue with publishers whose content is referenced in AI searches. "When Perplexity earns revenue from an interaction where a publisher's content is referenced, that publisher will also earn a share," the company explained.

The timing was pointed. Japanese newspapers had just filed copyright infringement lawsuits against Perplexity, and the company faced ongoing criticism from media outlets worldwide. The revenue-sharing model attempts to transform adversaries into partners.

"Perplexity AI Inc. is offering publishers the opportunity to share in the revenue their articles generate as the company looks to deal with criticism and legal action from some media outlets over use of their work," Bloomberg reported.

Whether publishers will accept such arrangements remains uncertain. The fundamental question—whether training AI on copyrighted content constitutes fair use—remains unresolved in the courts. The Copyright Alliance noted that "we are in the doldrums of discovery" with "no more fair use decisions expected until mid-to-late 2026."

## Smart Home Security Concerns

The month also highlighted security vulnerabilities in AI-powered consumer products. Researchers demonstrated how malicious hackers could exploit Google's Gemini AI to control smart homes through seemingly innocent Google Calendar invites.

"This is how malicious hackers could exploit Gemini AI to control a smart home," The Verge reported on August 10. The attack vector—passing instructions to AI through calendar invitations—illustrated how AI integration creates new attack surfaces.

The vulnerability reflects a broader problem: AI systems are being integrated into critical infrastructure before security implications are fully understood. The smart home case is relatively benign compared to AI integration in healthcare, finance, or industrial systems.

## The Bigger Picture

August 2025's disclosures painted a complex picture of AI's societal impact:

**Energy costs are real and significant.** Google's data confirmed what researchers suspected: AI inference is energy-intensive at scale. As AI usage grows, so does its environmental footprint.

**Misuse is happening now.** Anthropic's threat report documented actual criminal use of AI, not theoretical risks. The technology is already being weaponized.

**Compensation models are evolving.** Perplexity's publisher revenue sharing represents one attempt to address the value extraction problem. Others will follow.

**Security lags deployment.** AI systems are being integrated faster than security implications can be assessed. The smart home vulnerabilities are likely just the beginning.

The AI industry has long operated on the assumption that benefits outweigh costs. August 2025 forced a more honest accounting. The technology is powerful, but its environmental, security, and ethical implications demand serious attention.

The question isn't whether AI development should continue—it will. The question is whether the industry can develop the transparency, security practices, and compensation models needed to make that development sustainable.

---

*Sources: MIT Technology Review, Google Cloud Blog, arXiv, Anthropic, Reuters, Bloomberg, Digiday, Search Engine Journal, The Verge, Copyright Alliance*
