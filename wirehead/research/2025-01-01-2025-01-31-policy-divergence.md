# The Great Divergence: Global AI Policy Splits Between Regulation and Innovation

**Period: January 2025**  
**Impact: High**  
**Theme: Policy, Regulation, Governance, Legal**

## Executive Summary

January 2025 revealed a stark divergence in global AI governance approaches. While the European Union's AI Act entered its enforcement phase with prohibitions on high-risk systems, the United States under the Trump administration moved decisively toward deregulation. Meanwhile, copyright lawsuits proliferated and debates over training data transparency intensified. These conflicting approaches highlighted fundamental disagreements about how to balance AI innovation with safety, ethics, and established rights.

## EU AI Act: Prohibition Phase Begins

On February 2, 2025 (technically just after January but announced and prepared throughout the month), the EU's AI Act entered its first enforcement phase, prohibiting AI systems deemed to pose unacceptable risks. Banned applications included:

- **Social Scoring**: Systems evaluating or classifying people based on behavior
- **Predictive Policing**: AI making decisions about crime likelihood based on profiling
- **Real-Time Biometric Identification**: Facial recognition in public spaces (with narrow exceptions)
- **Emotion Recognition**: Systems in workplace and educational settings
- **Manipulative AI**: Systems designed to exploit vulnerabilities

The regulation represented the world's first comprehensive AI law, establishing a risk-based framework that would phase in additional requirements over 24 months. High-risk applications would face safety assessments, transparency obligations, and human oversight mandates.

European Commission officials emphasized the Act aimed to make AI "safe, trustworthy and compliant" while preserving innovation. Article 57 required each member state to establish AI regulatory sandboxes by August 2026, providing controlled environments for testing novel applications.

However, the approach drew criticism from some European AI companies. In July 2025 (beyond our January focus but reflecting sentiment building in January), a group of European tech CEOs including Mistral AI's Arthur Mensch would sign an open letter urging Brussels to "stop the clock" for two years. They argued excessive regulation risked handicapping European companies against less-constrained competitors.

## Trump Administration: The Deregulatory Turn

In sharp contrast, the Trump administration pursued aggressive deregulation. On January 23, President Trump signed Executive Order 14179 titled "Removing Barriers to American Leadership in Artificial Intelligence."

The order aimed to:

- **Revoke Previous AI Safety Requirements**: Eliminating Biden-era executive orders on AI safety
- **Establish Minimal Federal Framework**: Creating streamlined oversight rather than comprehensive regulation
- **Prevent State-Level Fragmentation**: Preempting state AI laws to avoid regulatory patchwork
- **Accelerate Innovation**: Removing perceived bureaucratic obstacles to AI development

A subsequent executive order in December 2025 would go further, explicitly seeking to prevent states from regulating AI and establish federal primacy. This reflected administration philosophy that innovation velocity outweighed precautionary approaches.

The deregulatory stance aligned with Stargate and other initiatives positioning AI as economic and strategic priority. Administration officials argued that overregulation risked ceding AI leadership to less-constrained competitors, particularly China.

Critics contended the approach neglected real risks around bias, privacy, labor displacement, and safety. They warned that absence of guardrails could lead to harmful applications and erosion of trust.

## State-Level Regulatory Attempts

Multiple US states attempted their own AI regulations, prompting federal preemption efforts. States explored:

- **Algorithmic Impact Assessments**: Requiring evaluation of AI systems' effects on protected groups
- **Automated Decision-Making Transparency**: Disclosure when AI makes consequential decisions
- **Sector-Specific Rules**: Regulations for AI in employment, housing, credit, and other domains
- **Data Privacy Protections**: State-level frameworks for AI training data

New York proposed particularly comprehensive legislation, though critics argued it would create "national fragmentation" despite claims of alignment. The tension between state innovation and federal uniformity remained unresolved, with legal challenges likely.

## Copyright Battles Intensify

Over 70 copyright infringement lawsuits against AI companies remained active as January 2025 began, with several high-profile developments expected throughout the year. The cases followed similar patterns:

**Plaintiff Arguments**:
- AI companies copied copyrighted works during training without authorization or compensation
- This constitutes infringement regardless of transformative use
- Training data should require licensing agreements
- Current practices undermine creative professionals' livelihoods

**Defense Arguments**:
- Training on publicly available data constitutes fair use
- AI-generated outputs don't substitute for copyrighted works
- Precedent supports learning from examples without licensing
- Requiring licenses would make AI development impractical

Cases spanned multiple creative domains:
- **Authors**: Major publishers and individual writers sued over book training data
- **Visual Artists**: Illustrators and photographers challenged image generation models
- **Programmers**: Developers questioned use of open-source code repositories
- **News Organizations**: Media companies sought compensation for article training data

Courts faced novel questions about whether existing copyright frameworks adequately addressed AI's unique characteristics. Rulings would profoundly affect AI development economics and open-source model viability.

The EFF and Copyright Alliance offered competing perspectives, with EFF emphasizing user rights and innovation while Copyright Alliance prioritized creator compensation and control.

## Training Data Transparency Push

EU AI Act requirements and copyright litigation drove increased transparency about training data sources. Several developments emerged:

**Documentation Requirements**: High-risk AI systems under EU regulation needed detailed data documentation
**Voluntary Disclosure**: Some companies published data cards describing training corpora
**Technical Standards**: Efforts to standardize how training data gets described and validated
**Audit Mechanisms**: Proposals for third-party verification of data provenance claims

However, commercial AI labs resisted full disclosure, citing competitive concerns and the impracticality of documenting internet-scale datasets. The tension between transparency demands and business realities remained acute.

## AI Safety Standards: Uneven Progress

The Future of Life Institute's Winter 2025 AI Safety Index revealed major AI companies partially aligned with emerging safety standards but lacked implementation depth. Key findings:

- **Inconsistent Practices**: Safety measures varied significantly across companies and applications
- **Limited Transparency**: Insufficient public information about safety testing and red-teaming
- **Benchmark Limitations**: Performance on abstract benchmarks didn't guarantee real-world safety
- **Resource Disparities**: Well-funded labs could afford comprehensive safety programs; smaller players struggled

The report highlighted that while frameworks like the EU's Code of Practice existed, actual implementation quality remained uneven. Calls grew for standardized safety requirements rather than voluntary commitments.

## China's Watermarking Requirements

China implemented AI-generated content watermarking requirements, mandating that synthetic media be clearly labeled. The policy aimed to combat misinformation and deepfakes while maintaining AI innovation.

Technical implementation challenges emerged:
- **Watermark Robustness**: Ensuring markers survived compression and editing
- **Detection Reliability**: Minimizing false positives and negatives  
- **International Coordination**: Whether watermarking could work globally given varying standards

The approach represented a middle path between prohibition and laissez-faire attitudes, attempting to enable AI use while mitigating specific harms.

## Philosophical Divergence

The January 2025 policy landscape revealed fundamental philosophical disagreements:

**Precautionary Approach** (EU, some US states):
- Regulate proactively before harms materialize
- Require safety demonstrations before deployment
- Prioritize protecting rights over maximizing innovation speed
- Accept that some beneficial applications may be delayed

**Innovation-First Approach** (US federal, China in some domains):
- Allow experimentation with rapid iteration
- Address problems reactively as they arise
- Prioritize maintaining competitive advantage
- Accept that some harms may occur during development

Neither approach was purely right or wrong—each involved tradeoffs. The EU accepted potential innovation slowdown to build trustworthy systems. The US bet that velocity would compound into insurmountable advantages while adapting to problems dynamically.

## Implications for Global AI Development

These divergent approaches created complex dynamics:

**Regulatory Arbitrage**: Companies could choose jurisdictions based on regulatory preference
**Fragmented Markets**: Different regions might access different AI capabilities
**Compliance Complexity**: Global companies faced conflicting requirements
**Innovation Distribution**: Whether regulation concentrated or dispersed AI development geographically

Some observers predicted regulatory competition would ultimately drive convergence toward middle-ground approaches. Others expected enduring divergence reflecting deeper cultural and political differences.

## Looking Forward

January 2025's policy developments set the stage for years of governance evolution. Key questions included:

- Would EU regulation prove compatible with innovation or stifle European AI?
- Could US deregulation deliver benefits without significant harms?
- How would copyright cases resolve, and would outcomes prove workable?
- Would international coordination emerge or would AI governance remain fragmented?

The answers would shape not just AI development but broader questions about technology governance, the relationship between innovation and regulation, and how democratic societies make decisions about transformative technologies.

For now, the divergence continued—a natural experiment in AI governance playing out across jurisdictions, with high stakes for both innovation and societal wellbeing.

---

**Sources**: European Commission, EU AI Act, White House, The New York Times, Copyright Alliance, EFF, Future of Life Institute, Reuters, Various Policy Sources

**Related Events**: Trump EO (#7), EU AI Act (#8), Copyright Lawsuits (#16), Data Transparency (#45), State Regulations (#49)
