# Regulation and Copyright Wars: The Legal Battle for AI's Future

**Period:** December 2025  
**Theme:** Legal Battles, Regulatory Actions, and Policy Shifts

## Executive Summary

December 2025 brought the legal and regulatory challenges of AI into sharp focus. New York Governor Kathy Hochul signed the RAISE Act, making New York the second state with major AI safety legislation. President Trump signed an executive order attempting to preempt state AI regulation, threatening federal lawsuits and funding cuts. Authors filed a major omnibus copyright lawsuit against six AI companies, arguing training on pirated books constituted "massive willful infringement." Adobe faced a class action over AI training data. The Pentagon partnered with xAI to deploy Grok across military systems. The month revealed a fundamental tension: as AI capabilities accelerated, legal frameworks struggled to keep pace, creating uncertainty that would shape the industry's trajectory for years.

## The State Regulation Battle

### New York's RAISE Act: Second State to Act

On December 20, Governor Kathy Hochul signed the RAISE Act (Responsible AI Safety and Enforcement), making New York the second state after California to enact major AI safety legislation.

The law's key provisions included:

**Safety Protocol Publication**: Developers of frontier AI models must publish safety frameworks describing how they prevent critical harms.

**72-Hour Incident Reporting**: Companies must report incidents of critical harm within 72 hours to the new oversight office.

**New DFS Office**: Creates a dedicated office within the New York State Department of Financial Services to enforce the law, issue regulations, assess fees, and publish annual reports.

**Significant Penalties**: Fines up to $1 million per violation, $3 million for repeat violations.

**Developer-Funded Enforcement**: The oversight office is funded by fees on developers themselves, not taxpayers.

The RAISE Act represented a unified benchmark among major tech states. With California and New York—home to most major AI companies—enacting similar frameworks, a de facto national standard was emerging from state action.

### Trump's Executive Order: Federal Preemption Attempt

On December 11, President Trump signed an executive order attempting to forestall state AI regulation by threatening federal lawsuits and withholding federal funds.

The order called for a "minimally burdensome national policy framework for AI" and created a taskforce whose "sole responsibility" would be challenging states' AI laws.

Key provisions:

**Federal Lawsuits**: Threatening legal action against states with "onerous" AI regulation
**Funding Threats**: Withholding some federal funds from non-compliant states
**National Framework**: Calling for unified federal approach
**Taskforce Creation**: Dedicated team to challenge state laws

However, the executive order lacked the force of law. It represented a policy position and threat, not binding legal authority. States could—and likely would—continue enacting their own AI regulations.

### The Federal-State Tension

The conflict between Trump's executive order and state actions like New York's RAISE Act highlighted a fundamental tension in AI governance:

**Federal Perspective**: Fragmented state regulations create compliance burdens, stifle innovation, and disadvantage US companies globally. A unified national framework would provide clarity and consistency.

**State Perspective**: Federal government has been slow to act on AI regulation. States have responsibility to protect residents from AI harms. State experimentation allows testing different approaches before national adoption.

The tension wasn't new—it echoed debates over privacy regulation (California's CCPA), data breach notification laws, and other technology policies where states acted before federal government.

But AI's rapid advancement and potential for both benefit and harm made the stakes higher. The question wasn't whether AI would be regulated, but at what level and with what approach.

## The Copyright Wars Intensify

### The Omnibus Author Lawsuit

On December 23, John Carreyrou and other prominent authors filed a major lawsuit against six AI companies: Anthropic, Google, OpenAI, Meta, xAI, and Perplexity.

The lawsuit alleged that all six companies trained their models on Books3, a dataset containing approximately 200,000 pirated books. The authors argued this constituted "massive willful infringement" that generated billions in revenue.

Key aspects of the lawsuit:

**Scope**: Targeted essentially the entire US AI industry
**Damages**: Potentially billions given the revenue generated by models trained on the data
**Precedent**: Built on earlier copyright lawsuits but expanded to cover more companies
**Timing**: Filed after Anthropic's $1.5B settlement with authors was deemed inadequate

The lawsuit represented a coordinated effort by authors to establish that training AI models on copyrighted works without permission or compensation violated copyright law—regardless of fair use arguments.

### The Legal Arguments

**Authors' Position**: Training AI models on copyrighted books without permission is copyright infringement. The fact that the books were pirated makes it willful infringement deserving enhanced damages. Fair use doesn't apply because:
- The use is commercial
- Entire works were copied
- The models compete with original works
- Authors receive no compensation

**AI Companies' Position**: Training on copyrighted works is fair use because:
- It's transformative (creating new capabilities, not reproducing works)
- It's analogous to human learning from reading
- Models don't store or reproduce copyrighted works verbatim
- It advances science and benefits society

The legal battle would likely take years to resolve, with potential appeals to the Supreme Court.

### Adobe's Class Action

On December 17, Adobe faced a proposed class action alleging the company trained its SlimLM model on SlimPajama-627B, a dataset containing Books3 derivatives.

The plaintiff's guidebooks appeared in the processed subset, and the lawsuit alleged training on copyrighted works without consent or compensation.

Adobe's case differed from others in one respect: the company had long positioned itself as creator-friendly, with tools for artists, designers, and writers. A lawsuit alleging pirated training data threatened that positioning.

### The Broader Copyright Landscape

December's lawsuits were part of a broader wave of copyright litigation against AI companies:

**Visual Artists**: Suing Stability AI, Midjourney, and others over image generation models
**News Publishers**: Suing OpenAI and Microsoft over article training data
**Music Industry**: Preparing lawsuits over music generation models
**Software Developers**: Questioning GitHub Copilot's use of open-source code

The pattern was clear: every creative industry was challenging AI companies' use of copyrighted training data.

## Military AI Deployment

### Pentagon Partners with xAI

On December 22, the Pentagon announced a partnership with Elon Musk's xAI to deploy Grok across government systems, integrating it into GenAI.mil alongside Google's Gemini.

The move marked a major expansion of military AI use and raised several questions:

**Security**: Could Grok handle classified information securely?
**Reliability**: Was the model sufficiently tested for military applications?
**Oversight**: What controls would govern military AI use?
**Precedent**: Would other AI companies gain similar military access?

The partnership reflected the Pentagon's strategy of adopting multiple AI models rather than relying on a single provider. In July 2025, the defense department had issued contracts worth up to $200 million each to Anthropic, Google, OpenAI, and xAI for developing AI agent systems.

### The Controversy

The xAI partnership proved controversial for several reasons:

**Musk's Conflicts**: As CEO of Tesla, SpaceX, and xAI, Musk had multiple government contracts and potential conflicts of interest.

**Grok's Reputation**: The model was known for fewer content restrictions than competitors, raising questions about appropriateness for military use.

**Rapid Deployment**: The timeline from contract to deployment seemed compressed compared to typical military procurement.

**Political Connections**: The partnership came shortly after Trump's election, with Musk having supported the campaign.

Critics argued the deployment prioritized political connections over rigorous evaluation. Supporters countered that rapid AI adoption was necessary for military competitiveness.

## The Global Regulatory Landscape

### Europe's AI Act Implementation

While not specific to December, the EU's AI Act continued its phased implementation:
- Governance rules and GPAI model obligations became applicable August 2, 2025
- High-risk AI system rules had extended transition until August 2, 2027

The Act represented the world's most comprehensive AI regulation, establishing risk-based categories and requirements.

### China's Approach

China's AI regulation evolved throughout 2025, with December developments including:

**DeepSeek Framework**: Publication of technical papers suggesting continued innovation despite chip restrictions
**IPO Preparations**: Zhipu and MiniMax filing for Hong Kong listings, navigating regulatory approval
**Narrowing Gap**: Analysis showing Chinese AI capabilities approaching US levels

China's approach combined:
- State control over AI development and deployment
- Support for domestic AI champions
- Restrictions on foreign AI models
- Investment in AI infrastructure despite chip limitations

### The Fragmented Global Landscape

By December 2025, AI regulation remained fragmented:

**United States**: State-level action (California, New York) with federal government divided
**European Union**: Comprehensive AI Act with phased implementation
**China**: State-controlled development with strategic support
**United Kingdom**: Principles-based approach with sector-specific guidance
**Other Regions**: Various stages of AI policy development

This fragmentation created challenges for global AI companies navigating different requirements across jurisdictions.

## The Trust and Adoption Paradox

December data revealed a troubling paradox: AI adoption was rising even as trust declined.

### The Stack Overflow Survey

Stack Overflow's December 29 developer survey showed:

**80% using AI tools** (up from previous years)
**29% trust in accuracy** (down from 40%)
**60% positive favorability** (down from 72%)
**66% spending more time** fixing "almost-right" AI code

The primary frustration: dealing with "AI solutions that are almost right, but not quite" (45% of respondents).

When code got complicated and stakes were high, 75% of developers still asked another person for help rather than trusting AI.

### The Enterprise Adoption Data

Menlo Ventures reported:

**$37B enterprise AI spending** in 2025 (up 3.2x)
**76% purchased vs built** (up from 53% in 2024)
**10+ products over $1B ARR**

But agent adoption lagged. Only 28% saw agents transforming team workflows, with 72% saying "vibe coding" wasn't part of professional work.

### The Regulatory Implications

The trust-adoption paradox had regulatory implications:

**Safety Concerns**: If users didn't trust AI but used it anyway, errors could have serious consequences
**Liability Questions**: Who was responsible when AI made mistakes—the user, the developer, or the model provider?
**Disclosure Requirements**: Should AI-generated content be labeled? (Sketchfab mandated "CreatedWithAI" labels starting December 11)
**Quality Standards**: Should AI systems meet minimum accuracy thresholds before deployment?

Regulators grappled with balancing innovation against safety, recognizing that overly restrictive rules could stifle beneficial development while inadequate rules could allow harmful deployment.

## The Scientific Output Question

On December 24, Cornell University published research in Science showing AI was supercharging scientific output while quality slipped.

The study found:
- **Output volume increasing** dramatically
- **Quality metrics declining** (citation impact, novelty)
- **Quantity-quality tradeoff** emerging

The findings raised regulatory questions:

**Research Integrity**: Should AI-assisted research be disclosed?
**Peer Review**: How should reviewers evaluate AI-generated content?
**Academic Standards**: What constitutes original contribution when AI assists?
**Funding Allocation**: Should grants prioritize human-led vs AI-assisted research?

The scientific community faced the same trust-adoption paradox as developers: AI tools were becoming ubiquitous even as concerns about quality grew.

## The Path Forward

December 2025's regulatory and legal developments revealed several emerging patterns:

### 1. State Action Filling Federal Void

With federal government divided on AI regulation, states like California and New York were establishing de facto national standards. This pattern would likely continue until federal legislation passed.

### 2. Copyright Battles Escalating

The omnibus author lawsuit and Adobe class action signaled that copyright litigation would intensify. The fundamental question—whether training on copyrighted works constituted fair use—would likely require Supreme Court resolution.

### 3. Military AI Expanding

The Pentagon's partnerships with multiple AI providers signaled that military AI use would expand rapidly, raising questions about oversight, safety, and appropriate use.

### 4. Trust-Adoption Gap Widening

As AI adoption grew while trust declined, pressure would mount for quality standards, disclosure requirements, and liability frameworks.

### 5. Global Fragmentation Persisting

Different jurisdictions would continue developing different AI regulatory approaches, creating compliance challenges for global companies.

## The Strategic Implications

For AI companies, December's regulatory landscape created several strategic imperatives:

**Regulatory Engagement**: Proactive engagement with regulators to shape frameworks rather than react to imposed rules

**Copyright Strategy**: Developing defensible positions on training data, potentially including licensing agreements with content creators

**Trust Building**: Investing in reliability, accuracy, and transparency to address declining trust

**Geographic Strategy**: Navigating fragmented global regulations while maintaining consistent product offerings

**Liability Preparation**: Preparing for potential liability as AI systems make consequential decisions

For policymakers, the challenge was balancing competing objectives:

**Innovation vs Safety**: Enabling beneficial AI development while preventing harmful deployment
**Federal vs State**: Coordinating across jurisdictions while allowing experimentation
**Speed vs Deliberation**: Acting quickly enough to address emerging risks while avoiding premature restrictions
**Competition vs Cooperation**: Maintaining competitive advantage while enabling international coordination

## Conclusion

December 2025 marked a turning point in AI regulation and legal battles. The industry had moved beyond the "move fast and break things" era into a period where legal and regulatory frameworks would increasingly shape what AI companies could build and how they could deploy it.

The RAISE Act and similar state legislation established that AI would be regulated, regardless of federal action. The copyright lawsuits signaled that training data practices would face legal scrutiny. The trust-adoption paradox suggested that quality and reliability would become competitive differentiators.

The companies that would succeed in this new environment wouldn't be those that moved fastest or broke most things. They would be those that:

1. **Engaged proactively** with regulators and policymakers
2. **Built defensible** training data practices
3. **Invested in reliability** and trust-building
4. **Navigated complexity** across fragmented jurisdictions
5. **Prepared for liability** as AI systems made consequential decisions

The legal and regulatory landscape of December 2025 wasn't the end of AI innovation—but it was the end of AI exceptionalism. The industry would be regulated like other consequential technologies, with all the complexity, cost, and constraint that entailed.

The question wasn't whether AI would transform society—but whether the legal and regulatory frameworks being built in December 2025 would enable that transformation responsibly or hinder it unnecessarily.

The answer would shape the next decade of AI development.

---

**Sources:**
- New York State: "Governor Hochul Signs Nation-Leading Legislation"
- Crowell & Moring: "Executive Order Tries to Thwart 'Onerous' AI State Regulation"
- Last Week in AI: "Authors file omnibus copyright lawsuit"
- TechCrunch: "Adobe faces class action lawsuit"
- ETC Journal: "Musk & Huang: What's Up? (Dec. 2025)"
- Stack Overflow: "2025 Developer Survey results"
- Menlo Ventures: "2025: The State of Generative AI in the Enterprise"
- ScienceDaily: "AI supercharges scientific output while quality slips"
- Game Developer: "Sketchfab to require mandatory AI disclosure"
