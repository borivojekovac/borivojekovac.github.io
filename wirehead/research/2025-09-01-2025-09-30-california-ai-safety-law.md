# California Becomes First US State to Regulate Frontier AI with SB 53

*September 29, 2025*

In a landmark move that could reshape the future of artificial intelligence governance in the United States, California Governor Gavin Newsom signed Senate Bill 53 (SB 53), the Transparency in Frontier Artificial Intelligence Act, into law on September 29, 2025. The legislation makes California the first state in the nation to establish enforceable regulatory requirements for the most advanced AI systems.

## A First-of-Its-Kind Framework

The new law, authored by State Senator Scott Wiener, requires developers of frontier AI models to publish detailed safety frameworks explaining how they manage "catastrophic risks." It also provides whistleblower protections for employees who report safety concerns—a provision that gained Anthropic's support after weeks of negotiations with the AI industry.

"California is home to the world's leading AI companies and talent," Governor Newsom said in a statement. "With this legislation, we're ensuring that innovation continues responsibly, with appropriate transparency and accountability."

The timing is significant. In May, House Republicans buried a ten-year federal preemption of state AI regulation in the Budget Reconciliation bill - a provision that would have rendered SB 53 unenforceable before it was even signed. But in a rare bipartisan rebuke, the Senate struck the preemption 99-1 in July, with Senators Maria Cantwell (D-WA) and Marsha Blackburn (R-TN) co-sponsoring the amendment. With Washington's blockade lifted, California stepped into the regulatory vacuum. According to the 2025 Stanford AI Index, 15.7% of all US AI job postings are in California - well ahead of Texas (8.8%) and New York (5.8%). More than half of global venture capital funding for AI and machine learning startups went to Bay Area companies in 2024.

## What the Law Requires

SB 53 targets "frontier" AI models—the most powerful systems developed by companies like OpenAI, Anthropic, Google, and Meta. Key requirements include:

- **Safety Frameworks**: Developers must publish detailed documentation on how they identify, assess, and mitigate catastrophic risks
- **Transparency Reports**: Regular public disclosures about model capabilities, limitations, and safety testing results
- **Whistleblower Protections**: Employees who report safety violations are protected from retaliation
- **Incident Reporting**: Companies must notify regulators of significant safety incidents

The law draws on policy principles established by the Biden administration's 2023 AI Executive Order and incorporates lessons from the EU AI Act's implementation.

## Industry Reactions: A Divided Response

The AI industry's response has been notably split. Anthropic, after initially expressing concerns, came out in support of the bill following negotiations that strengthened whistleblower provisions. "We believe transparency requirements, when properly designed, can advance both safety and innovation," the company said in a statement.

However, not everyone in Silicon Valley is pleased. Peter Thiel, the influential tech investor, offered perhaps the most extreme criticism, arguing that regulating AI would "literally be the Antichrist." His comments reflect a broader libertarian strain in tech circles that views any government oversight as fundamentally hostile to innovation.

Senator Ted Cruz, meanwhile, introduced competing federal legislation that would allow AI companies to set their own rules for up to 10 years—a stark contrast to California's approach.

## The Newsom Factor

The signing comes at a politically charged moment for Newsom, who is widely seen as positioning himself for a presidential run. The governor has significant ties to Silicon Valley donors, and OpenAI has reportedly staffed up with Newsom-affiliated political operatives. Some observers questioned whether he would sign the bill at all.

"We're about to find out if Silicon Valley owns Gavin Newsom," wrote Brian Merchant in his newsletter *Blood in the Machine* earlier in September, as the bill awaited the governor's signature.

In the end, Newsom chose to sign, though he emphasized that the law strikes a balance between safety and innovation. "California's AI industry will continue to lead the world," he said. "This law ensures that leadership is responsible."

## Implications for the AI Industry

The Carnegie Endowment for International Peace called SB 53 "a blueprint for evidence-generating transparency measures that could shape the next few years of frontier AI governance." As the EU refines its AI Act implementation and China continues evolving its safety governance frameworks, California's approach offers a potential model for other jurisdictions.

For AI companies, the law creates new compliance obligations but also provides regulatory clarity that has been lacking at the federal level. Companies operating in California—which includes virtually all major AI developers—will need to adapt their practices regardless of what happens in Washington.

The law takes effect in phases, with initial requirements beginning January 1, 2026. Full implementation is expected by mid-2026.

## A Turning Point?

SB 53 represents a significant shift in the AI governance landscape. For years, the industry has operated with minimal regulatory oversight, relying largely on voluntary commitments and self-regulation. California's law signals that era may be ending.

Whether other states follow California's lead—or whether Congress finally acts on federal legislation—remains to be seen. But for now, the Golden State has established itself as the de facto regulator of America's most powerful AI systems.

---

*Sources: California Governor's Office, Brookings Institution, Carnegie Endowment for International Peace, The Verge, Wharton AI Analytics*
